\documentclass[12pt]{article}

\usepackage[toc,page]{appendix}

\usepackage[utf8]{inputenc}

\usepackage{tikz}
\usetikzlibrary{calc}

\usepackage{enumerate}

\usepackage{etoolbox}
\AtBeginEnvironment{quote}{\small}

\usepackage{fontspec}
\setmainfont[Ligatures=TeX]{Liberation Serif}
\setsansfont{Liberation Sans}
\setmonofont{Liberation Mono}

\usepackage{mathtools}
\DeclarePairedDelimiter{\floor}{\lfloor}{\rfloor}

\usepackage{polyglossia}
\setdefaultlanguage{english}

\usepackage[colorlinks=true,urlcolor=blue,linkcolor=blue]{hyperref}

\begin{document}

\title{CASPaxos: Replicated State Machines without logs}
\date{30 January 2018}
\author{Denis Rystsov \\
\href{mailto:rystsov.denis@gmail.com}{rystsov.denis@gmail.com}}


\maketitle

\begin{abstract}
CASPaxos is a Single Decree Paxos (Synod) based protocol for building a replicated state machine. Unlike Raft and Multi-Paxos, it doesn't use an idea of the leader election and the replicated log, so it avoids associated complexity (e.g. log compaction) and provides graceful performance degradation when replicas are crashed.

The lightweight nature of CASPaxos opens ways for a new application for RSM, e.g. instead of representing a key/value storage as a single RSM; it's possible to run an instance of CASPaxos per key and achieve a result similar to EPaxos.

This paper describes CASPaxos protocol, proves its safety properties, covers cluster membership change and evaluates a CASPaxos-based key/value storage against established consistent databases.
\end{abstract}


\section{Introduction}

Multi-Paxos\cite{lamport01} and Raft\cite{raft} protocols allow a collection of machines to work as a coherent group - a replicated state machine (RSM). The protocols preserve liveness when at least $\floor{N/2} + 1$ of $N$ machines are up and connected, preserve safety in the presence of arbitrary crash/recovery and loss of messages.

The problem of keeping RSM work when its nodes are falling apart is isomorphic to the problem of master-master replication of a linearizable distributed storage under the same conditions. So those protocols are widely used in the industry as a foundation of such databases as Chubby\cite{chubby}, Etcd\footnote{\href{https://github.com/coreos/etcd}{https://github.com/coreos/etcd}}, Spanner\cite{spanner}, etc.

Despite the wide adoption, there are a lot of indications that the protocols are complex. Diego Ongaro and John Ousterhout, authors of Raft, wrote in their paper:

\begin{quote}
In an informal survey of attendees at NSDI 2012, we found few people who were comfortable with Paxos, even among seasoned researchers. We struggled with Paxos ourselves; we were not able to understand the complete protocol until after reading several simplified explanations and designing our own alternative protocol, a process that took almost a year
\end{quote}

Google's engineers wrote about their experience of building a Paxos-based database in the "Paxos Made Live"\cite{chubby} paper:

\begin{quote}
Despite the existing literature in the field, building such a database proved to be non-trivial \ldots{} While Paxos can be described with a page of pseudo-code, our complete implementation contains several thousand lines of C++ code \ldots{} There are significant gaps between the description of the Paxos algorithm and the needs of a real-world system.
\end{quote}

The complexity of RSM protocols may lead to errors in implementations. Kyle Kingsbury made a comprehensive research\footnote{\href{https://aphyr.com/tags/jepsen}{https://aphyr.com/tags/jepsen}} of distributed consistent databases and found violations of linearizability in almost every database he tested including MongoDB, Etcd, Consul, RethinkDB, VoltDB and CockroachDB.

{\bf Contributions.} I present CASPaxos, a novel protocol for building RSM that avoids complexities of Multi-Paxos and Raft. Multi-Paxos is RSM built on top of a replicated log in which each log entry is a command. The replicated log is modelled as an array of instances of Single Decree Paxos. According to D. Ongaro and J. Ousterhout its complexity comes from the composition rules:

\begin{quote}
We hypothesize that Paxos’ opaqueness derives from its choice of the single-decree subset as its foundation \ldots{} The composition rules for Multi-Paxos add significant additional complexity and subtlety.

One reason is that there is no widely agreed upon algorithm for multi-Paxos. Lamport’s descriptions are mostly about single-decree Paxos; he sketched possible approaches to multi-Paxos, but many details are missing. As a result, practical systems bear little resemblance to Paxos. Each implementation begins with Paxos, discovers the difficulties in implementing it, and then develops a significantly different architecture \ldots{} real implementations are so different from Paxos that the proofs have little value
\end{quote}

CASPaxos extends Synod, a write-once distributed register, into a rewritable distributed register (which is isomorphic to a RSM) instead of using it as a building block, so there is no composition and the associated complexity.

According to an experimental study\cite{raft} and the number of open source implementations\footnote{\href{https://raft.github.io/\#implementations}{https://raft.github.io/\#implementations}} Raft is more understandable than Multi-Paxos. However, its complexity is comparable with the latter: both systems\cite{chubby}\cite{raft} have several thousand of lines of code, both use concepts of leader, leader election, leases, both are based on logs and require log compaction. CASPaxos is significantly simpler: it doesn't have those pieces and its implementation\footnote{\href{https://github.com/gryadka/js}{https://github.com/gryadka/js}} is less than 500 lines of code.

Being just an extension of Synod, CASPaxos uses its symmetric peer-to-peer approach and automatically achieves the goals set in the EPaxos\cite{epaxos} paper: (1) optimal commit latency in the wide-area when tolerating one and two failures, under realistic conditions; (2) uniform load balancing across all replicas (thus achieving high throughput); and (3) graceful performance degradation when replicas are slow or crash.

The formal proof is included into the appendix, TLA+ models were independently written by Tobias Schottdorf\footnote{\href{https://tschottdorf.github.io/single-decree-paxos-tla-compare-and-swap}{https://tschottdorf.github.io/single-decree-paxos-tla-compare-and-swap}} and Greg Rogers\footnote{\href{https://medium.com/@grogepodge/tla-specification-for-gryadka-c80cd625944e}{https://medium.com/@grogepodge/tla-specification-for-gryadka-c80cd625944e}} and they didn't reveal any linearizability violation.

In the following sections, I describe the CASPaxos protocol, cluster membership change and evaluate a CASPaxos-based key/value storage.

\section{Algorithm}

We begin by briefly describing the Synod protocol from the perspective of master-master replication, followed by an overview of its extension into CASPaxos.

\subsection{Synod}

Synod protocol allows to build a distributed register which a client can initialize only once. If several clients try to initialize a register concurrently then the requests either prevent each other from continuing or a single initialization succeeds. Once a client recieved confirmation all the follow up initializations must result with a conflict and return the initialized value.

The system belongs to the CP-specter of CAP theorem and keeps working without compromising safety when at least $\floor{N/2} + 1$ of $N$ nodes are up and connected; with more failures it compromises availability.

The roles of nodes in the system:
\begin{enumerate}
  \item {\bf Clients} initiate a request by communicating with a proposer, client may be stateless, the system may have arbitrary numbers of clients.
  \item {\bf Proposers} perform the initialization by communicating with acceptors. Proposers keep state to generate unique increasing update ID (ballot number), the system may have arbitrary numbers of proposers.
  \item {\bf Acceptors} store the accepted value, the system should have $2F+1$ acceptor to tolerate $F$ failures.
\end{enumerate}

\begin{figure}[!h]
  \centering
  \begin{tikzpicture}[y=-1cm]
    \node at (0,-0.5)[scale=0.8]{Client};
    \node at (2,-0.5)[scale=0.8]{Proposer};
    \node at (4.5,-0.5)[scale=0.8]{Acceptor A};
    \node at (6.5,-0.5)[scale=0.8]{Acceptor B};
    \node at (8.5,-0.5)[scale=0.8]{Acceptor C};
    
    \draw (0,0) -- (0,5.6);
    \draw (2,0) -- (2,5.6);
    \draw (4.5,0) -- (4.5,5.6);
    \draw (6.5,0) -- (6.5,2.05);
    \draw (8.5,0) -- (8.5,5.6);
    \draw (6.4,2.05) -- (6.6,2.05);
  
    \draw[dotted] (-0.2,0.75) -- (8.6,0.75);
    \draw[dotted] (-0.2,2.55) -- (8.6,2.55);
    \draw[dotted] (-0.2,4.05) -- (8.6,4.05);
  
    \begin{scope}[very thick]
      \draw[->] (0,0.5) -- (2,0.5) node[above, midway, scale=0.8]{set 3};
      
      \node at (1,1.6)[scale=0.8]{Propose};
      \draw[->] (2,1) -- (4.5,1);
      \draw[->] (2,1.3) -- (6.5,1.3);
      \draw[->] (2,1.6) -- (8.5,1.6);
      \draw[<-, dashed] (2,1.9) -- (4.5,1.9);
      \draw[<-, dashed] (2,2.2) -- (8.5,2.2);
  
      \node at (1,3.35)[scale=0.8]{Accept};
      \draw[->] (2,2.9) -- (4.5,2.9);
      \draw[->] (2,3.2) -- (8.5,3.2);
      \draw[<-, dashed] (2,3.5) -- (4.5,3.5);
      \draw[<-, dashed] (2,3.8) -- (8.5,3.8);
  
      \draw[<-, dashed] (0,4.3) -- (2,4.3) node[below, midway, scale=0.8]{ok};
    \end{scope}
  \end{tikzpicture}
\end{figure}

The initialization procedure:
\begin{enumerate}
  \item A client proposes a value to a proposer.
  \item The proposer generates a ballot number, $B$, and send a prepare message containing that number to the acceptors.
  \item An acceptor:
  \begin{itemize}
    \item Returns a conflict if it already saw a greater ballot number.
    \item Persists the ballot number as a promise and returns a confirmation either with an empty value (if it hasn't accepted any yet) or with a tuple of an accepted value and its ballot number.
  \end{itemize}
  \item The proposer waits for the $F+1$ confirmations
  \begin{itemize}
    \item If they all contain an empty value then the proposer writes "ok" to the result variable and sends an accept message containing the ballot number $B$ and the proposing value to the acceptors.
    \item If at least one message contains a tuple then the proposer picks the tuple with the highest ballot number, writes ("conflict", the tuple's value) to the result variable and sends an accept message with the generated ballot number $B$ and the tuple's value to the acceptors.
  \end{itemize}
  \item An acceptor:
  \begin{itemize}
    \item Returns a conflict if it already saw a greater ballot number.
    \item Erases the promise, marks the recieved (ballot number, value) as the accepted value and returns a confirmation
  \end{itemize}
  \item The proposer waits for the $F+1$ confirmations
  \item The proposer returns the result variable to the client
\end{enumerate}

A proposer may combine an increasing counter with its numerical ID to generate a tuple (counter, ID) and use it as the ballot number. To compare the ballot tuples we should compare the counter parts and use ID only as a tie breaker. When a proposer recieves a conflict from the acceptors it may fastforward its counter to avoid the conflict for the next request.

\subsection{CASPaxos}

CASPaxos is about a rewritable distributed register. It stores state, clients change it by submitting a pure function which takes the current state as an argument and yields new as a return value. Out of concurrent requests only one can succeed, once a client gets a confirmation of the change it's guranteed that all future states are its decendants: there exists a chain of changes linking them together.

Just like Synod it's a CP-system and it requires $2F+1$ nodes to tolerate $F$ failures. Just like the Synod it uses the same roles: clients, proposers and acceptors, and a very similar two phase change mechanism.

The change procedure:
\begin{enumerate}
  \item A client submits a change function to a proposer.
  \item The proposer generates a ballot number, $B$, and send a prepare message containing that number to the acceptors.
  \item An acceptor:
  \begin{itemize}
    \item Returns a conflict if it already saw a greater ballot number.
    \item Persists the ballot number as a promise and returns a confirmation either with an empty value (if it doesn't have any state yet) or with a tuple of a latest value and its ballot number.
  \end{itemize}
  \item The proposer waits for the $F+1$ confirmations
  \begin{itemize}
    \item The proposer defines the current state as $\emptyset$ if each confirmation contains an empty value; if at least one confirmation contains a tuple then the proposer picks the value of the tuple with the highest ballot number as current state.
    \item Then it invokes the change function passing the current state as an argument to calculate the new state and sends it with the generated ballot number $B$ as an accept message to the acceptors.
  \end{itemize}
  \item An acceptor:
  \begin{itemize}
    \item Returns a conflict if it already saw a greater ballot number.
    \item Erases the promise, marks the recieved (ballot number, value) as the accepted value and returns a confirmation
  \end{itemize}
  \item The proposer waits for the $F+1$ confirmations.
  \item The proposer returns the new state to the client.
\end{enumerate}

As you can see the change procedure is almost identical to the Synod's initialization procedure. And if we use "$x \to \mbox{if}\; x = \emptyset \;\mbox{then}\; val\; \mbox{else}\; x$" as the change function then it becomes identical to Synod's attempt to initialize register with the $val$ value.

We may use the following change functions to turn CASPaxos into a distributed compare and set register:
\begin{enumerate}
  \item To {\bf initialize} a register with $val_0$ value: "$x \to \mbox{if}\; x = \emptyset \;\mbox{then}\; (0, val_0)\; \mbox{else}\; x$"
  \item To {\bf update} a register to value $val_1$ if the current version is $5$: "$x \to \mbox{if}\; x = (5, \ast) \;\mbox{then}\; (6, val_1)\; \mbox{else}\; x$"
  \item To {\bf read} a value: "$x \to x$"
\end{enumerate}

\subsubsection{One-round trip optimisation}

Since the prepare phase doesn't depend on the change function it's possible to piggyback the next prepare message on the current accept message to reduce the number of round trips from to two to one.

\subsubsection{Cluster membership change}

From practical perspective it's necessary to have a way to change cluster membership to replace failed nodes. 

CASPaxos’s mechanism for cluster membership change is based on Raft's idea of joint consensus\cite{raft} approach where the majorities of two different configurations overlap during transitions. This allows the cluster to continue operating normally during configuration changes.

The proof of this idea with application to CASPaxos is based of two observation:

\begin{enumerate}
  \item {\bf Reduced quorums}. The results from the "Flexible Paxos: Quorum intersection revisited"\cite{fpaxos} paper are applicable to CASPaxos (see proof in appendix \ref{appendix:fpaxos}). In particular the majority of confirmations for the prepare and the accept phases isn't necessary and the only requirement is intersection. For example, if the cluster size is $4$ then we can require only $2$ confirmations for the prepare phase if we wait for $3$ confirmations for the accept phase.
  
  \item {\bf Filter equivalence} If a change in the behavior of the CASPaxos cluster can be explained by delaying or omitting the messages between the nodes of the cluster then the change doesn't affect consistency since it tolerates the interventions of such kind. It gives freedom in changing the system as long as the change can be modeled as a message filter on top of the unmodified system.
\end{enumerate}

The protocol for changing the set of acceptors from $A_1 \cdots A_{2f+1}$ to $A_1 \cdots A_{2f+2}$:
\begin{enumerate}
  \item Turn on the $A_{2f+2}$ acceptor.
  \item Connect to each proposer and update its configuration to send the accept messages to the $A_1 \cdots A_{2f+2}$ set of acceptors and to require $f+2$ confirmations.
  \item Pick any proposer and execute the identity change function $x \to x$.
  \item Connect to each proposer and update its configuration to send prepare messages to the $A_1 \cdots A_{2f+2}$ set of acceptors and to require $f+2$ confirmations.
\end{enumerate}

From the perspective of $2f+1$ nodes cluster the second step can be explained with the filter equivalence, so the system keeps being correct. The third step is just a read operation so it doesn't change the correctness either but after it's finished we can treat the system as a valid $2f+2$ nodes cluster with the reduced prepare quorum. The last step switches the system from the reduced to the regular $2f+2$ nodes cluster mode.

The protocol for changing the set of acceptors from $A_1 \cdots A_{2f+2}$ to $A_1 \cdots A_{2f+3}$ is simpler because we can treat $2f+2$ nodes cluster as $2f+3$ nodes cluster where one node is down:
\begin{enumerate}
  \item Connect to each proposer and update its configuration to send the prepare \& accept messages to the $A_1 \cdots A_{2f+3}$ set of acceptors.
  \item Turn on the $A_{2f+3}$ acceptor.
\end{enumerate}

To reduce the size of the cluster the same steps should be executed in the reverse order.

\subsubsection{Low-latency and high-throughput consensus across WAN deployments}

WPaxos\cite{wpaxos} paper describes how to achieve low-latency and high-throughput consensus across widearea network through object stealing. It leverages the flexible quorums\cite{fpaxos} idea to cut WAN communication costs. Since CASPaxos being an extension of Synod provides flexible quorums capabilities it can benefit from the same idea.

\section{A CASPaxos-based key/value storage}

The lightweight nature of CASPaxos opens new simple ways for designing distributed systems with complex behavior. In this section we'll discuss a CASPaxos-based design for a key/value storage and compare a research prototype with established databases.

The Raft paper acknoledges\cite{raft} that EPaxos\cite{epaxos} can achieve higher performance than Raft by using leaderless approach and exploiting commutativity in state machine commands. A key/value storage with independent operations between keys looks like a good case for the EPaxos protocol. However it adds significant complexity.

Alternativly, instead of putting the whole key/value storage under a single RSM and using the commutativity of the commands, we can use lightweight nature of CASPaxos and run a RSM per key to achieve uniform load balancing across all replicas (thus higher throughput).

Gryadka\footnote{\href{https://github.com/gryadka/js}{https://github.com/gryadka/js}} is a prototype of distributed key/value storage which uses that approach.

\subsection{How to delete a record}

The proposed variant of CASPaxos supports only update (change) operation so to delete a value a client should update a register with an empty value. The downside of this approach is the space inefficiency, even when the value is empty the system still spends space to maintain information about the register: a promise and an associated ballot number.

A straightforward removal of this information may introduce consistency issues. Consider the following state of the acceptors.

\begin{figure}[!h]
  \centering
  \begin{tabular}{ r|r|r|r }
    & Promise & Ballot & State \\ \hline
    Acceptor A && 2 & 42 \\
    Acceptor B && 3 & $\emptyset$ \\
    Acceptor C && 3 & $\emptyset$ \\
  \end{tabular}
\end{figure}

According to the CASPaxos propocol a read operation (implemented as $x \to x$ change function) should return $\emptyset$. However, if we decide to remove all the information accociated with the register and a request hits the system during the removal when the data on acceptors B and C has already gone then the outcome is $42$ which violates linearizability.

An incresing of accept quorum to $2F+1$ on writing an empty value before the removal solves the problem but it makes the system less available. A multi-step removal process can be used to avoid the availability impact.

\begin{enumerate}
  \item On a delete request a system writes an empty value with regular $F+1$ accept-quorum, schedules a garbage collection operation and confirms the request to a client.
  \item The garbage collection operation:
  \begin{enumerate}
    \item Replcates an empty value to all nodes by reading it with $2F+1$ quorum size.
    \item Removes the empty register from every acceptor.
  \end{enumerate}
\end{enumerate}

\subsection{Evaluation}
\subsubsection{Fault-tolerance}

\subsubsection{Performance}
\section{Conclusion}

\begin{appendices}
\section{Proof}
\href{http://rystsov.info/2015/09/16/how-paxos-works.html}{http://rystsov.info/2015/09/16/how-paxos-works.html}

\section{FPaxos}
\label{appendix:fpaxos}
\href{http://rystsov.info/2015/12/30/read-write-quorums.html}{http://rystsov.info/2015/12/30/read-write-quorums.html}

\end{appendices}

\newpage

\begin{thebibliography}{9}

\bibitem{lamport01}
  Leslie Lamport,
  \emph{"Paxos Made Simple"}.
  2001.

\bibitem{raft}
  Diego Ongaro, John Ousterhout
  \emph{"In Search of an Understandable Consensus Algorithm"}.
  2013.

\bibitem{epaxos}
  Iulian Moraru, David G. Andersen, Michael Kaminsky
  \emph{"There Is More Consensus in Egalitarian Parliaments"}.
  2013.

\bibitem{chubby}
  Tushar Chandra, Robert Griesemer, Joshua Redstone
  \emph{"Paxos Made Live - An Engineering Perspective"}.
  2007.

\bibitem{spanner}
  Corbett, J. C., Dean, J., Epstein, M., Fikes, A., Frost C., Furman, J.J., Ghemawat, S., Gubarev, A., Heiser, C., Hochschild, P., at al.
  \emph{"Spanner: Googles globally distributed database"}.
  2012.

\bibitem{fpaxos}
  Heidi Howard, Dahlia Malkhi, Alexander Spiegelman
  \emph{Flexible Paxos: Quorum intersection revisited}.
  2016.

\bibitem{wpaxos}
  Ailidani Ailijiang, Aleksey Charapko, Murat Demirbas, Tevfik Kosar
  \emph{WPaxos: Ruling the Archipelago with Fast Consensus}.
  2017.

\end{thebibliography}

\end{document}